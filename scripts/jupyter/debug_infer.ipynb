{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "os.environ['HF_HUB_CACHE'] = '/next_share/hf_cache/hub/'\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, PreTrainedTokenizer, AutoModelForCausalLM\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig\n",
    ")\n",
    "from accelerate import PartialState, Accelerator\n",
    "\n",
    "import context\n",
    "os.chdir(context.proj_dir)\n",
    "\n",
    "from cont_gen.data_loader.cuad_prompt import CUAD_SFT, SFT_Padding, CUAD_SFT_Seq2Seq\n",
    "from cont_gen.data_loader.cuad_sft import CUAD_SFT_Cached, CUAD_SFT_Filter_Type\n",
    "from cont_gen.utils.model_utils import build_hf_or_peft_model, smart_resize_embeddings, load_hf_model_from_checkpoint\n",
    "from cont_gen.trainer.utils import get_smart_optimizer, compute_clm_loss_with_ignore\n",
    "from cont_gen.trainer.train_only_accelerate import Trainer_Basic, TrainingArgs_Basic\n",
    "from cont_gen.model.loss import LM_Simple_Feed\n",
    "from cont_gen.run.infer_sft import SimpleGenerator, load_test_dataset\n",
    "from cont_gen.data_process.utils import tokenize_wo_bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "def build_tkn(path):\n",
    "    return AutoTokenizer.from_pretrained(path, trust_remote_code = True)\n",
    "\n",
    "# tokenizer name to path\n",
    "TKN_MAP = {\n",
    "    'flan-t5': build_tkn('google/flan-t5-large'),\n",
    "    'llama2': build_tkn('meta-llama/Llama-2-7b-hf'),\n",
    "    'llama3': build_tkn('meta-llama/Meta-Llama-3-8B'),\n",
    "    'mistral': build_tkn('mistralai/Mistral-7B-v0.1'),\n",
    "    # 'phi1': build_tkn('microsoft/phi-1_5'),\n",
    "    'phi2': build_tkn('microsoft/phi-2')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tk_name='llama3'\n",
    "model_path = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "is_seq2seq = False\n",
    "ckpt = 'runs/ood/llama3_chat/seed42_tr29/pmt_01_all_lr1e-5_bs16_wd0.0/checkpoint-15692'\n",
    "\n",
    "data_path = f'data/ood_split/seed42_tr29/{tk_name}/pmt_01/train_data.jsonl'\n",
    "# tokenizer = TKN_MAP[tk_name]\n",
    "tokenizer = build_tkn(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Ä world', '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenize_wo_bos(tokenizer, 'Hello world.').input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128009]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['<|eot_id|>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta-llama/Meta-Llama-3-8B-Instruct'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_name='mistral'\n",
    "model_path = 'mistralai/Mistral-7B-v0.1'\n",
    "is_seq2seq = False\n",
    "# ckpt = 'runs/ood/llama3/seed42_tr29/pmt_01_lr1e-5_bs16_wd0.0/checkpoint-15692'\n",
    "\n",
    "data_path = f'data/ood_split/seed42_tr29/{tk_name}/pmt_01/train_data.jsonl'\n",
    "tokenizer = TKN_MAP[tk_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from cache: data/ood_split/seed42_tr29/llama3/pmt_01/cache/cached_train_data.jsonl_Meta-Llama-3-8B-Instruct_v1.1chat.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "tr_ds = CUAD_SFT_Cached(data_path, tokenizer, is_seq2seq = is_seq2seq,\n",
    "                        is_chat = True, \n",
    "                          cache_dir = Path(data_path).parent / 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a0cde49c7c4532844897047d926326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6664280410934e30b6c411b798779912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'model' in dir():\n",
    "    del model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype = torch.bfloat16, device_map = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "adp_st = torch.load(Path(ckpt) / 'adapter_model.bin', map_location='cpu')\n",
    "print([k for k in adp_st.keys() if 'layers' not in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'base_model.model.model.embed_tokens.weight' in adp_st:\n",
    "    new_shape = adp_st['base_model.model.model.embed_tokens.weight'].shape[0]\n",
    "    model.resize_token_embeddings(new_shape)\n",
    "model.load_adapter(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda(data):\n",
    "    return {k:v.cuda() for k,v in data.items()}\n",
    "\n",
    "def to_batch(data):\n",
    "    return {k:torch.tensor(v).unsqueeze(0).cuda() for k,v in data.items()}\n",
    "\n",
    "def add_target_head(sample, head):\n",
    "    \"\"\"Append target head to source\"\"\"\n",
    "    ori_ids = sample['input_ids']\n",
    "    ipt_len = len([k for k in sample['labels'] if k == -100])\n",
    "    new_ids = ori_ids[:ipt_len] + list(head)\n",
    "    new_mask = [1] * len(new_ids)\n",
    "    new_labels = [-100] * ipt_len + list(head)\n",
    "    return {'input_ids': new_ids, 'attention_mask': new_mask, 'labels': new_labels}\n",
    "\n",
    "def generate(model, batch, tokenizer, max_len = 512):\n",
    "    return model.generate(**{k:batch[k] for k in ['input_ids', 'attention_mask']},\n",
    "                          do_sample = False, eos_token_id = tokenizer.eos_token_id,\n",
    "                          max_new_tokens = max_len)\n",
    "\n",
    "def get_prob(logits, top_k = 5):\n",
    "    probs= torch.softmax(logits, dim = -1)\n",
    "    rank = torch.argsort(probs, descending = True).tolist()\n",
    "    return [(rank[i], probs[rank[i]].item()) for i in range(top_k)]\n",
    "\n",
    "def get_token_prob(logits, tokenizer, top_k = 5):\n",
    "    top = get_prob(logits, top_k)\n",
    "    return [(tokenizer.convert_ids_to_tokens(tid), tid, p) for tid, p in top]\n",
    "\n",
    "def greedy_generate(model, batch, tokenizer, num_token = 5):\n",
    "    past_key_values = []\n",
    "    input_ids = batch['input_ids']\n",
    "    mask = batch['attention_mask']\n",
    "    gen_tokens = []\n",
    "    for step in range(num_token):\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids = input_ids, attention_mask = mask, past_key_values = past_key_values)\n",
    "        top = get_token_prob(out.logits[0][-1], tokenizer)\n",
    "        print(f'Step {step+1}')\n",
    "        for token, tid, p in top:\n",
    "            print(f'\\t{token} {tid} {p:.4f}')\n",
    "        past_key_values = out.past_key_values\n",
    "        input_ids = torch.tensor([[top[0][1]]]).cuda()\n",
    "        mask = torch.concat([mask, torch.tensor([[1]]).cuda()], dim = -1)\n",
    "        gen_tokens.append(top[0][1])\n",
    "    return gen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage_fast/rhshui/lib/anaconda3/envs/llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/storage_fast/rhshui/lib/anaconda3/envs/llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sample = tr_ds[0]\n",
    "\n",
    "prompt = add_target_head(sample, [])\n",
    "pmt_len = len(prompt['input_ids'])\n",
    "with torch.no_grad():\n",
    "    ba = to_batch(prompt)\n",
    "    out = model(**ba)\n",
    "    gen_out = generate(model, ba, tokenizer, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' an energy efficiency device, which is referred to as an \"Energy Saver\" which may be improved or otherwise changed from its present composition (the \"Products\"). The Company may engage in the business of selling other products or other devices other than the Products, which will be considered Products if Distributor exercises its options pursuant to Section 7 hereof.\\n\\n###Question: The date of the contract\\n\\n###Answer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n- 7th day of September, 1999.<|end_of_text|>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample['input_ids'][-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "\t- 12 0.8060\n",
      "\tNo 2822 0.1401\n",
      "\t7 22 0.0313\n",
      "\tThis 2028 0.0062\n",
      "\tthis 576 0.0054\n",
      "Step 2\n",
      "\tÄ this 420 0.7154\n",
      "\tÄ  220 0.2050\n",
      "\tÄ September 6250 0.0518\n",
      "\tÄ The 578 0.0102\n",
      "\tÄ This 1115 0.0079\n",
      "Step 3\n",
      "\tÄ  220 0.9996\n",
      "\t<|end_of_text|> 128001 0.0002\n",
      "\tÄ 198 0.0001\n",
      "\tÄ ? 949 0.0000\n",
      "\t? 30 0.0000\n",
      "Step 4\n",
      "\t7 22 1.0000\n",
      "\tÄ  220 0.0000\n",
      "\tÄ seventh 31487 0.0000\n",
      "\t6 21 0.0000\n",
      "\t17 1114 0.0000\n",
      "Step 5\n",
      "\tth 339 0.9999\n",
      "\t< 27 0.0000\n",
      "\t(th 25364 0.0000\n",
      "\t^ 61 0.0000\n",
      "\tÄ th 270 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12, 420, 220, 22, 339]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_generate(model, to_batch(prompt), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits', 'past_key_values'])\n",
      "205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(out.keys())\n",
    "print(len(prompt['input_ids']))\n",
    "out.logits[0][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    12,    420,    220,     22,    339,   1938,    315,   6250,     11,\n",
      "           220,   2550,     24,     13, 128001, 128006, 128006,  78191, 128007,\n",
      "           271,     40], device='cuda:0')\n",
      "['-', 'Ä this', 'Ä ', '7', 'th', 'Ä day', 'Ä of', 'Ä September', ',', 'Ä ', '199', '9', '.', '<|end_of_text|>', '<|start_header_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ÄÄ', 'I']\n"
     ]
    }
   ],
   "source": [
    "gen_tokens = gen_out[0][pmt_len:]\n",
    "print(gen_tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(gen_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|eot_id|>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<0x0A>', 13, 0.4116723835468292),\n",
       " ('â', 28705, 0.2496919333934784),\n",
       " ('âSeptember', 4074, 0.10408708453178406),\n",
       " ('âThe', 415, 0.0593070350587368),\n",
       " ('â\"', 345, 0.026317333802580833)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_prob(out.logits[0][-1], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considered Products if Distributor exercises its options pursuant to Section 7 hereof.\n",
      "\n",
      "###Question: The date of the contract\n",
      "\n",
      "###Answer: - 7th day of September, 1999.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(sample['input_ids'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'Ä world']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TKN_MAP['phi2'].convert_ids_to_tokens(TKN_MAP['phi2'].encode('hello world'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model.model.embed_tokens.weight[128:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6055, 0.6016, 0.6289, 0.6289, 0.6289, 0.6055, 0.6211],\n",
       "       device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(vec, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(model.model.embed_tokens.weight, dim = 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââ| 11808/11808 [00:06<00:00, 1753.68it/s]\n"
     ]
    }
   ],
   "source": [
    "test_ds = CUAD_SFT_Filter_Type(\n",
    "        'data/ood_split/seed42_tr29/llama3/pmt_01/test_data_ood.jsonl',\n",
    "        tokenizer,\n",
    "        is_seq2seq = is_seq2seq,\n",
    "        is_chat = True,\n",
    "        # cache_dir = Path(args.data_path).parent / 'cache',\n",
    "        is_test = True,\n",
    "        judge_type_fn = lambda k: k>0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant. Review the contract clauses and answer questions. Output the mentioned clauses if exist; otherwise output \"No\".\n",
      "\n",
      "###Clauses:\n",
      "Exhibit 10.16 SUPPLY CONTRACT Contract No: Date: The buyer/End-User: Shenzhen LOHAS Supply Chain Management Co., Ltd. ADD: Tel No. : Fax No. : The seller: ADD: The Contract is concluded and signed by the Buyer and Seller on, in Hong Kong. 1. General provisions 1.1 This is a framework agreement, the terms and conditions are applied to all purchase orders which signed by this agreement (hereinafter referred to as the \"order\"). 1.2 If the provisions of the agreement are inconsistent with the order, the order shall prevail. Not stated in order content will be subject to the provisions of agreement. Any modification, supplementary, give up should been written records, only to be valid by buyers and sellers authorized representative signature and confirmation, otherwise will be deemed invalid. 2. The agreement and order 2.1 During the validity term of this agreement, The buyer entrust SHENZHEN YICHANGTAI IMPORT AND EXPORT TRADE CO., LTD or SHENZHEN LEHEYUAN TRADING CO, LTD (hereinafter referred to as the \"entrusted party\" or \"YICHANGTAI\" or \"LEHEYUAN\"), to purchase the products specified in this agreement from the seller in the form of orders. 2.2 The seller shall be confirmed within three working days after receipt of order. If the seller finds order is not acceptable or need to modify, should note entrusted party in two working days after receipt of the order, If the seller did not confirm orders in time or notice not accept orders or modifications, the seller is deemed to have been accepted the order. The orders become effective once the seller accepts, any party shall not unilaterally cancel the order before the two sides agreed. 2.3 If the seller puts forward amendments or not accept orders, the seller shall be in the form of a written notice to entrusted party, entrusted party accept the modified by written consent, the modified orders to be taken effect. 2.4 Seller's note, only the buyer entrust the entrusted party issued orders, the product delivery and payment has the force of law.\n",
      "\n",
      "###Question: The name of the contract\n",
      "\n",
      "###Answer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(test_ds[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
