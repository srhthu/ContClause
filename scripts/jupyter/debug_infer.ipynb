{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "os.environ['HF_HUB_CACHE'] = '/next_share/hf_cache/hub/'\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, PreTrainedTokenizer, AutoModelForCausalLM\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig\n",
    ")\n",
    "from accelerate import PartialState, Accelerator\n",
    "\n",
    "import context\n",
    "os.chdir(context.proj_dir)\n",
    "\n",
    "from cont_gen.data_loader.cuad_prompt import CUAD_SFT, SFT_Padding, CUAD_SFT_Seq2Seq\n",
    "from cont_gen.data_loader.cuad_sft import CUAD_SFT_Cached, CUAD_SFT_Filter_Type\n",
    "from cont_gen.utils.model_utils import build_hf_or_peft_model, smart_resize_embeddings, load_hf_model_from_checkpoint\n",
    "from cont_gen.trainer.utils import get_smart_optimizer, compute_clm_loss_with_ignore\n",
    "from cont_gen.trainer.train_only_accelerate import Trainer_Basic, TrainingArgs_Basic\n",
    "from cont_gen.model.loss import LM_Simple_Feed\n",
    "from cont_gen.run.infer_sft import SimpleGenerator, load_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "def build_tkn(path):\n",
    "    return AutoTokenizer.from_pretrained(path, trust_remote_code = True)\n",
    "\n",
    "# tokenizer name to path\n",
    "TKN_MAP = {\n",
    "    'flan-t5': build_tkn('google/flan-t5-large'),\n",
    "    'llama2': build_tkn('meta-llama/Llama-2-7b-hf'),\n",
    "    'llama3': build_tkn('meta-llama/Meta-Llama-3-8B'),\n",
    "    'mistral': build_tkn('mistralai/Mistral-7B-v0.1'),\n",
    "    # 'phi1': build_tkn('microsoft/phi-1_5'),\n",
    "    'phi2': build_tkn('microsoft/phi-2')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tk_name='llama3'\n",
    "model_path = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "is_seq2seq = False\n",
    "ckpt = 'runs/ood/llama3/seed42_tr29/pmt_01_lr1e-5_bs16_wd0.0/checkpoint-15692'\n",
    "\n",
    "data_path = f'data/ood_split/seed42_tr29/{tk_name}/pmt_01/train_data.jsonl'\n",
    "# tokenizer = TKN_MAP[tk_name]\n",
    "tokenizer = build_tkn(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_name='mistral'\n",
    "model_path = 'mistralai/Mistral-7B-v0.1'\n",
    "is_seq2seq = False\n",
    "# ckpt = 'runs/ood/llama3/seed42_tr29/pmt_01_lr1e-5_bs16_wd0.0/checkpoint-15692'\n",
    "\n",
    "data_path = f'data/ood_split/seed42_tr29/{tk_name}/pmt_01/train_data.jsonl'\n",
    "tokenizer = TKN_MAP[tk_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 15812/15812 [00:11<00:00, 1423.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to cache: data/ood_split/seed42_tr29/mistral/pmt_01/cache/cached_train_data.jsonl_Mistral-7B-v0.1_v1.1.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "tr_ds = CUAD_SFT_Cached(data_path, tokenizer, is_seq2seq = is_seq2seq,\n",
    "                        is_chat = True, \n",
    "                          cache_dir = Path(data_path).parent / 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309621dd6f8f4653a52275ae5b725226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'model' in dir():\n",
    "    del model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype = torch.bfloat16, device_map = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base_model.model.model.embed_tokens.weight', 'base_model.model.lm_head.weight']\n"
     ]
    }
   ],
   "source": [
    "adp_st = torch.load(Path(ckpt) / 'adapter_model.bin', map_location='cpu')\n",
    "print([k for k in adp_st.keys() if 'layers' not in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shape = adp_st['base_model.model.model.embed_tokens.weight'].shape[0]\n",
    "model.resize_token_embeddings(new_shape)\n",
    "model.load_adapter(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda(data):\n",
    "    return {k:v.cuda() for k,v in data.items()}\n",
    "\n",
    "def to_batch(data):\n",
    "    return {k:torch.tensor(v).unsqueeze(0).cuda() for k,v in data.items()}\n",
    "\n",
    "def add_target_head(sample, head):\n",
    "    \"\"\"Append target head to source\"\"\"\n",
    "    ori_ids = sample['input_ids']\n",
    "    ipt_len = len([k for k in sample['labels'] if k == -100])\n",
    "    new_ids = ori_ids[:ipt_len] + list(head)\n",
    "    new_mask = [1] * len(new_ids)\n",
    "    new_labels = [-100] * ipt_len + list(head)\n",
    "    return {'input_ids': new_ids, 'attention_mask': new_mask, 'labels': new_labels}\n",
    "\n",
    "def generate(model, batch, tokenizer, max_len = 512):\n",
    "    return model.generate(**{k:batch[k] for k in ['input_ids', 'attention_mask']},\n",
    "                          do_sample = False, eos_token_id = tokenizer.eos_token_id,\n",
    "                          max_new_tokens = max_len)\n",
    "\n",
    "def get_prob(logits, top_k = 5):\n",
    "    probs= torch.softmax(logits, dim = -1)\n",
    "    rank = torch.argsort(probs, descending = True).tolist()\n",
    "    return [(rank[i], probs[rank[i]].item()) for i in range(top_k)]\n",
    "\n",
    "def get_token_prob(logits, tokenizer, top_k = 5):\n",
    "    top = get_prob(logits, top_k)\n",
    "    return [(tokenizer.convert_ids_to_tokens(tid), tid, p) for tid, p in top]\n",
    "\n",
    "def greedy_generate(model, batch, tokenizer, num_token = 5):\n",
    "    past_key_values = []\n",
    "    input_ids = batch['input_ids']\n",
    "    mask = batch['attention_mask']\n",
    "    gen_tokens = []\n",
    "    for step in range(num_token):\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids = input_ids, attention_mask = mask, past_key_values = past_key_values)\n",
    "        top = get_token_prob(out.logits[0][-1], tokenizer)\n",
    "        print(f'Step {step+1}')\n",
    "        for token, tid, p in top:\n",
    "            print(f'\\t{token} {tid} {p:.4f}')\n",
    "        past_key_values = out.past_key_values\n",
    "        input_ids = torch.tensor([[top[0][1]]]).cuda()\n",
    "        mask = torch.concat([mask, torch.tensor([[1]]).cuda()], dim = -1)\n",
    "        gen_tokens.append(top[0][1])\n",
    "    return gen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tr_ds[0]\n",
    "\n",
    "prompt = add_target_head(sample, [])\n",
    "pmt_len = len(prompt['input_ids'])\n",
    "# with torch.no_grad():\n",
    "#     ba = to_batch(prompt)\n",
    "#     out = model(**ba)\n",
    "#     gen_out = generate(model, ba, tokenizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "\t<|begin_of_text|> 128000 0.9891\n",
      "\tĠ 220 0.0005\n",
      "\tĠNo 2360 0.0004\n",
      "\tĠDonovan 71800 0.0002\n",
      "\t=\"\" 8573 0.0002\n",
      "Step 2\n",
      "\tNo 2822 0.9608\n",
      "\tĠ 220 0.0031\n",
      "\tThe 791 0.0024\n",
      "\tĊ 198 0.0015\n",
      "\tQuestion 14924 0.0014\n",
      "Step 3\n",
      "\t<|end_of_text|> 128001 0.9990\n",
      "\tĊ 198 0.0003\n",
      "\t. 13 0.0002\n",
      "\tĊĊ 271 0.0001\n",
      "\t<|begin_of_text|> 128000 0.0001\n",
      "Step 4\n",
      "\t<|begin_of_text|> 128000 1.0000\n",
      "\t<|end_of_text|> 128001 0.0000\n",
      "\tĠon 389 0.0000\n",
      "\tĠMe 2206 0.0000\n",
      "\t{ 90 0.0000\n",
      "Step 5\n",
      "\tNo 2822 0.9657\n",
      "\tĠ 220 0.0033\n",
      "\tThe 791 0.0022\n",
      "\tĊ 198 0.0015\n",
      "\t- 12 0.0012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[128000, 2822, 128001, 128000, 2822]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_generate(model, to_batch(prompt), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits', 'past_key_values'])\n",
      "196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128257])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(out.keys())\n",
    "print(len(prompt['input_ids']))\n",
    "out.logits[0][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([128000,   2822, 128001], device='cuda:0')\n",
      "['<|begin_of_text|>', 'No', '<|end_of_text|>']\n"
     ]
    }
   ],
   "source": [
    "gen_tokens = gen_out[0][pmt_len:]\n",
    "print(gen_tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(gen_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<0x0A>', 13, 0.4116723835468292),\n",
       " ('▁', 28705, 0.2496919333934784),\n",
       " ('▁September', 4074, 0.10408708453178406),\n",
       " ('▁The', 415, 0.0593070350587368),\n",
       " ('▁\"', 345, 0.026317333802580833)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_prob(out.logits[0][-1], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considered Products if Distributor exercises its options pursuant to Section 7 hereof.\n",
      "\n",
      "###Question: The date of the contract\n",
      "\n",
      "###Answer: - 7th day of September, 1999.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(sample['input_ids'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'Ġworld']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TKN_MAP['phi2'].convert_ids_to_tokens(TKN_MAP['phi2'].encode('hello world'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model.model.embed_tokens.weight[128:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6055, 0.6016, 0.6289, 0.6289, 0.6289, 0.6055, 0.6211],\n",
       "       device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(vec, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(model.model.embed_tokens.weight, dim = 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 11808/11808 [00:06<00:00, 1753.68it/s]\n"
     ]
    }
   ],
   "source": [
    "test_ds = CUAD_SFT_Filter_Type(\n",
    "        'data/ood_split/seed42_tr29/llama3/pmt_01/test_data_ood.jsonl',\n",
    "        tokenizer,\n",
    "        is_seq2seq = is_seq2seq,\n",
    "        is_chat = True,\n",
    "        # cache_dir = Path(args.data_path).parent / 'cache',\n",
    "        is_test = True,\n",
    "        judge_type_fn = lambda k: k>0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant. Review the contract clauses and answer questions. Output the mentioned clauses if exist; otherwise output \"No\".\n",
      "\n",
      "###Clauses:\n",
      "Exhibit 10.16 SUPPLY CONTRACT Contract No: Date: The buyer/End-User: Shenzhen LOHAS Supply Chain Management Co., Ltd. ADD: Tel No. : Fax No. : The seller: ADD: The Contract is concluded and signed by the Buyer and Seller on, in Hong Kong. 1. General provisions 1.1 This is a framework agreement, the terms and conditions are applied to all purchase orders which signed by this agreement (hereinafter referred to as the \"order\"). 1.2 If the provisions of the agreement are inconsistent with the order, the order shall prevail. Not stated in order content will be subject to the provisions of agreement. Any modification, supplementary, give up should been written records, only to be valid by buyers and sellers authorized representative signature and confirmation, otherwise will be deemed invalid. 2. The agreement and order 2.1 During the validity term of this agreement, The buyer entrust SHENZHEN YICHANGTAI IMPORT AND EXPORT TRADE CO., LTD or SHENZHEN LEHEYUAN TRADING CO, LTD (hereinafter referred to as the \"entrusted party\" or \"YICHANGTAI\" or \"LEHEYUAN\"), to purchase the products specified in this agreement from the seller in the form of orders. 2.2 The seller shall be confirmed within three working days after receipt of order. If the seller finds order is not acceptable or need to modify, should note entrusted party in two working days after receipt of the order, If the seller did not confirm orders in time or notice not accept orders or modifications, the seller is deemed to have been accepted the order. The orders become effective once the seller accepts, any party shall not unilaterally cancel the order before the two sides agreed. 2.3 If the seller puts forward amendments or not accept orders, the seller shall be in the form of a written notice to entrusted party, entrusted party accept the modified by written consent, the modified orders to be taken effect. 2.4 Seller's note, only the buyer entrust the entrusted party issued orders, the product delivery and payment has the force of law.\n",
      "\n",
      "###Question: The name of the contract\n",
      "\n",
      "###Answer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(test_ds[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
