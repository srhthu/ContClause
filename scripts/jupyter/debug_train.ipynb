{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage_fast/rhshui/lib/anaconda3/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, PreTrainedTokenizer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig\n",
    ")\n",
    "from accelerate import PartialState, Accelerator\n",
    "\n",
    "import context\n",
    "\n",
    "from cont_gen.trainer.utils_dist import initialize_accelerator, DistLogger\n",
    "from cont_gen.data_loader.cuad_prompt import CUAD_SFT, SFT_Padding, CUAD_SFT_Seq2Seq\n",
    "from cont_gen.data_loader.cuad_sft import CUAD_SFT_Cached\n",
    "from cont_gen.utils.model_utils import build_hf_or_peft_model, smart_resize_embeddings\n",
    "from cont_gen.trainer.utils import get_smart_optimizer, compute_clm_loss_with_ignore\n",
    "from cont_gen.trainer.train_only_accelerate import Trainer_Basic, TrainingArgs_Basic\n",
    "from cont_gen.model.loss import LM_Simple_Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cont_gen.run.train_sft import (\n",
    "    load_train_dataset, build_model, get_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/storage_fast/rhshui/workspace/contract_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_parser().parse_args([])\n",
    "args.__dict__.update(dict(\n",
    "    total_batch_size = 16,\n",
    "    data_path = 'data/ood_split/seed42_tr29/pmt_01/train_data.jsonl',\n",
    "    base_model = '/storage_fast/rhshui/llm/ms-phi-1_5/',\n",
    "    dtype = 'bf16',\n",
    "    lr = 1e-5,\n",
    "    weight_decay = 0.0,\n",
    "    device_batch_size = 1,\n",
    "    max_epochs = 1,\n",
    "    logging_steps = 5\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-25 01:22:00][Main] Training args:\n",
      " {\n",
      "    \"output_dir\": null,\n",
      "    \"ds_config\": null,\n",
      "    \"debug\": false,\n",
      "    \"total_batch_size\": 16,\n",
      "    \"data_path\": \"data/ood_split/seed42_tr29/pmt_01/train_data.jsonl\",\n",
      "    \"max_length\": 512,\n",
      "    \"labels_on_full\": false,\n",
      "    \"base_model\": \"/storage_fast/rhshui/llm/ms-phi-1_5/\",\n",
      "    \"saved_model\": null,\n",
      "    \"dtype\": \"bf16\",\n",
      "    \"lora\": false,\n",
      "    \"lora_r\": 8,\n",
      "    \"lora_alpha\": 16,\n",
      "    \"lora_target_modules\": null,\n",
      "    \"lora_dropout\": 0.05,\n",
      "    \"lr\": 1e-05,\n",
      "    \"weight_decay\": 0.0,\n",
      "    \"device_batch_size\": 1,\n",
      "    \"grad_acc_steps\": 16,\n",
      "    \"max_epochs\": 1,\n",
      "    \"max_steps\": null,\n",
      "    \"logging_steps\": 5,\n",
      "    \"save_steps\": null,\n",
      "    \"save_epochs\": 1,\n",
      "    \"save_total_limit\": 5,\n",
      "    \"local_rank\": null\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 14886/14886 [00:09<00:00, 1597.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to cache: data/ood_split/seed42_tr29/pmt_01/cache/cached_train_data.jsonl_ms-phi-1_5_v1.0.pkl\n",
      "[2024-04-25 01:22:16][Main] Not resize embeddings. model: 51200, tokenizer: 50296\n"
     ]
    }
   ],
   "source": [
    "state = PartialState()\n",
    "# grad acc step\n",
    "if args.grad_acc_steps is None:\n",
    "    args.grad_acc_steps = int(args.total_batch_size / args.device_batch_size / state.num_processes)\n",
    "\n",
    "accelerator, ds_config = initialize_accelerator(\n",
    "    args.ds_config, args.device_batch_size, args.grad_acc_steps\n",
    ")\n",
    "\n",
    "# Get logger\n",
    "log_file = None if args.output_dir is None else str(Path(args.output_dir) / 'log.txt')\n",
    "logger = DistLogger(file = log_file)\n",
    "\n",
    "## log arguments\n",
    "args_str = json.dumps(args.__dict__, indent = 4, ensure_ascii=False)\n",
    "if args.output_dir:\n",
    "    with open(Path(args.output_dir) / 'args.json', 'w') as f:\n",
    "        f.write(args_str)\n",
    "logger.log(f'Training args:\\n {args_str}')\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.base_model, trust_remote_code = True)\n",
    "if \"pad_token\" not in tokenizer.special_tokens_map:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Build dataset\n",
    "is_seq2seq = ('t5' in args.base_model)\n",
    "dataset = load_train_dataset(args, tokenizer)\n",
    "\n",
    "## data collate fn\n",
    "pad_args = {'pad_side': 'right'}\n",
    "if args.debug:\n",
    "    pad_args['pad_to_max_len'] = args.max_length if is_seq2seq else args.max_length*2\n",
    "collate_fn = SFT_Padding(tokenizer.pad_token_id, **pad_args)\n",
    "\n",
    "# Build model\n",
    "model = build_model(args, accelerator, logger)\n",
    "\n",
    "## resize model embedding if necessary\n",
    "smart_resize_embeddings(tokenizer, model, logger)\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "## build optimizer\n",
    "optimizer = get_smart_optimizer(model, args.lr, args.weight_decay)\n",
    "\n",
    "# Build trainer\n",
    "## training args\n",
    "tr_args = TrainingArgs_Basic(\n",
    "    device_batch_size = args.device_batch_size,\n",
    "    grad_acc_steps=args.grad_acc_steps,\n",
    "    max_epochs = args.max_epochs,\n",
    "    max_steps = args.max_steps,\n",
    "    logging_steps = args.logging_steps,\n",
    "    save_steps = args.save_steps,\n",
    "    save_epochs = args.save_epochs,\n",
    "    save_total_limit = args.save_total_limit\n",
    ")\n",
    "assert tr_args.total_batch_size == args.total_batch_size\n",
    "\n",
    "## Trainer\n",
    "trainer = Trainer_Basic(\n",
    "    tr_args, model, dataset, optimizer, accelerator,\n",
    "    ds_config = ds_config,\n",
    "    collate_fn = collate_fn,\n",
    "    compute_loss_fn = LM_Simple_Feed(),\n",
    "    output_dir = args.output_dir,\n",
    "    logger = logger\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "[1639, 389, 257, 7613, 8796, 13, 6602, 262, 2775, 31485, 290, 3280, 2683, 13, 25235, 262, 4750, 31485, 611, 2152, 26, 4306, 5072, 366, 2949, 1911, 198, 198, 21017, 47404, 2664, 25, 198, 6369, 39, 9865, 2043, 838, 13, 21, 198, 34957, 9865, 3843, 1581, 13077, 2200, 12529, 198, 12680, 34957, 9865, 3843, 1581, 13077, 2200, 12529, 357, 1169, 366, 10262, 10237, 4943, 318, 925, 416, 290, 1022, 13944, 2254, 11421, 1539, 257, 19603, 12017, 5855, 39154, 4943, 290, 13944, 2254, 286, 9486, 11419, 5855, 20344, 2455, 273, 4943, 428, 767, 400, 1110, 286, 2693, 11, 7358, 13, 198, 19644, 2043, 23333, 198, 317, 13, 383, 5834, 338, 7320, 13, 383, 5834, 318, 27606, 7953, 287, 262, 1597, 286, 6301, 281, 2568, 9332, 3335, 11, 543, 318, 6412, 284, 355, 281, 366, 28925, 311, 8770, 1, 543, 743, 307, 6596, 393, 4306, 3421, 422, 663, 1944, 11742, 357, 1169, 366, 48650, 11074, 383, 5834, 743, 8209, 287, 262, 1597, 286, 6301, 584, 3186, 393, 584, 4410, 584, 621, 262, 18675, 11, 543, 481, 307, 3177, 18675, 611, 46567, 273, 13565, 663, 3689, 12997, 284, 7275, 767, 994, 1659, 13, 198, 198, 21017, 24361, 25, 383, 3128, 286, 262, 2775, 198, 198, 21017, 33706, 25, 12, 767, 400, 1110, 286, 2693, 11, 7358, 13, 50256]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 12, 767, 400, 1110, 286, 2693, 11, 7358, 13, 50256]\n",
      "You are a helpful assistant. Review the contract clauses and answer questions. Output the mentioned clauses if exist; otherwise output \"No\".\n",
      "\n",
      "###Clauses:\n",
      "EXHIBIT 10.6\n",
      " DISTRIBUTOR AGREEMENT\n",
      " THIS DISTRIBUTOR AGREEMENT (the \"Agreement\") is made by and between Electric City Corp., a Delaware corporation (\"Company\") and Electric City of Illinois LLC (\"Distributor\") this 7th day of September, 1999.\n",
      " RECITALS\n",
      " A. The Company's Business. The Company is presently engaged in the business of selling an energy efficiency device, which is referred to as an \"Energy Saver\" which may be improved or otherwise changed from its present composition (the \"Products\"). The Company may engage in the business of selling other products or other devices other than the Products, which will be considered Products if Distributor exercises its options pursuant to Section 7 hereof.\n",
      "\n",
      "###Question: The date of the contract\n",
      "\n",
      "###Answer:- 7th day of September, 1999.<|endoftext|>\n",
      "- 7th day of September, 1999.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "print(sample.keys())\n",
    "print(sample['input_ids'])\n",
    "print(sample['labels'])\n",
    "print(tokenizer.decode(sample['input_ids']))\n",
    "print(tokenizer.decode([k for k in sample['labels'] if k != -100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_d = dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "9\n",
      "204\n"
     ]
    }
   ],
   "source": [
    "src_enc = tokenizer(raw_d['source'])\n",
    "tgt_enc = tokenizer(raw_d['target'])\n",
    "print(len(src_enc.input_ids))\n",
    "print(len(tgt_enc.input_ids))\n",
    "\n",
    "input_ids = src_enc.input_ids\n",
    "input_ids = input_ids + tgt_enc.input_ids\n",
    "print(len(src_enc.input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
