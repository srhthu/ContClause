{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "os.environ['HF_HUB_CACHE'] = '/next_share/hf_cache/hub/'\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, PreTrainedTokenizer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig\n",
    ")\n",
    "from accelerate import PartialState, Accelerator\n",
    "\n",
    "import context\n",
    "os.chdir(context.proj_dir)\n",
    "\n",
    "from cont_gen.trainer.utils_dist import initialize_accelerator, DistLogger\n",
    "from cont_gen.data_loader.cuad_prompt import CUAD_SFT, SFT_Padding, CUAD_SFT_Seq2Seq\n",
    "from cont_gen.data_loader.cuad_sft import CUAD_SFT_Cached\n",
    "from cont_gen.utils.model_utils import build_hf_or_peft_model, smart_resize_embeddings, load_hf_model_from_checkpoint\n",
    "from cont_gen.trainer.utils import get_smart_optimizer, compute_clm_loss_with_ignore\n",
    "from cont_gen.trainer.train_only_accelerate import Trainer_Basic, TrainingArgs_Basic\n",
    "from cont_gen.model.loss import LM_Simple_Feed\n",
    "from cont_gen.run.infer_sft import SimpleGenerator, load_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cont_gen.run.train_sft import (\n",
    "    load_train_dataset, build_model, get_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_name='llama3'\n",
    "model_path = 'meta-llama/Meta-Llama-3-8B'\n",
    "ckpt = 'runs/ood/llama3/seed42_tr29/pmt_01_lr1e-5_bs16_wd0.0/checkpoint-15692'\n",
    "args = get_parser().parse_args([])\n",
    "args.__dict__.update(dict(\n",
    "    total_batch_size = 16,\n",
    "    data_path = f'data/ood_split/seed42_tr29/{tk_name}/pmt_01/train_data.jsonl',\n",
    "    base_model = model_path,\n",
    "    dtype = 'bf16',\n",
    "    lr = 1e-5,\n",
    "    weight_decay = 0.0,\n",
    "    device_batch_size = 1,\n",
    "    max_epochs = 1,\n",
    "    logging_steps = 5\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from cache: data/ood_split/seed42_tr29/llama3/pmt_01/cache/cached_train_data.jsonl_Meta-Llama-3-8B_v1.0.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f896fb1501fa4f08a7716e51a806a5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resize embedding num to 128257\n"
     ]
    }
   ],
   "source": [
    "state = PartialState()\n",
    "# grad acc step\n",
    "if args.grad_acc_steps is None:\n",
    "    args.grad_acc_steps = int(args.total_batch_size / args.device_batch_size / state.num_processes)\n",
    "\n",
    "accelerator, ds_config = initialize_accelerator(\n",
    "    args.ds_config, args.device_batch_size, args.grad_acc_steps\n",
    ")\n",
    "\n",
    "# Get logger\n",
    "log_file = None\n",
    "logger = DistLogger(file = log_file)\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.base_model, trust_remote_code = True)\n",
    "if \"pad_token\" not in tokenizer.special_tokens_map:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Build dataset\n",
    "is_seq2seq = ('t5' in args.base_model)\n",
    "dataset = load_train_dataset(args, tokenizer)\n",
    "\n",
    "## data collate fn\n",
    "pad_args = {'pad_side': 'right'}\n",
    "if args.debug:\n",
    "    pad_args['pad_to_max_len'] = args.max_length if is_seq2seq else args.max_length*2\n",
    "collate_fn = SFT_Padding(tokenizer.pad_token_id, **pad_args)\n",
    "\n",
    "# Build model\n",
    "# model = build_model(args, accelerator, logger)\n",
    "# Load from checkpoint\n",
    "model = load_hf_model_from_checkpoint(ckpt, accelerator, args.dtype)\n",
    "\n",
    "## resize model embedding if necessary\n",
    "# smart_resize_embeddings(tokenizer, model, logger)\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "## build optimizer\n",
    "optimizer = get_smart_optimizer(model, args.lr, args.weight_decay)\n",
    "\n",
    "# Build trainer\n",
    "## training args\n",
    "tr_args = TrainingArgs_Basic(\n",
    "    device_batch_size = args.device_batch_size,\n",
    "    grad_acc_steps=args.grad_acc_steps,\n",
    "    max_epochs = args.max_epochs,\n",
    "    max_steps = args.max_steps,\n",
    "    logging_steps = args.logging_steps,\n",
    "    save_steps = args.save_steps,\n",
    "    save_epochs = args.save_epochs,\n",
    "    save_total_limit = args.save_total_limit\n",
    ")\n",
    "assert tr_args.total_batch_size == args.total_batch_size\n",
    "\n",
    "## Trainer\n",
    "trainer = Trainer_Basic(\n",
    "    tr_args, model, dataset, optimizer, accelerator,\n",
    "    ds_config = ds_config,\n",
    "    collate_fn = collate_fn,\n",
    "    compute_loss_fn = LM_Simple_Feed(),\n",
    "    output_dir = args.output_dir,\n",
    "    logger = logger\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "[128000, 2675, 527, 264, 11190, 18328, 13, 10506, 279, 5226, 50198, 323, 4320, 4860, 13, 9442, 279, 9932, 50198, 422, 3073, 26, 6062, 2612, 330, 2822, 11690, 14711, 65217, 4881, 512, 3337, 39, 3336, 964, 220, 605, 13, 21, 198, 98941, 878, 76590, 16837, 198, 10245, 98941, 878, 76590, 16837, 320, 1820, 330, 9219, 17589, 909, 374, 1903, 555, 323, 1990, 21246, 4409, 22621, 2637, 264, 40838, 27767, 3573, 14831, 909, 323, 21246, 4409, 315, 19174, 15620, 3573, 35, 79488, 909, 420, 220, 22, 339, 1938, 315, 6250, 11, 220, 2550, 24, 627, 75236, 34288, 50, 198, 362, 13, 578, 8351, 596, 8184, 13, 578, 8351, 374, 50801, 17045, 304, 279, 2626, 315, 11486, 459, 4907, 15374, 3756, 11, 902, 374, 14183, 311, 439, 459, 330, 33775, 328, 7403, 1, 902, 1253, 387, 13241, 477, 6062, 5614, 505, 1202, 3118, 18528, 320, 1820, 330, 18219, 1865, 578, 8351, 1253, 16988, 304, 279, 2626, 315, 11486, 1023, 3956, 477, 1023, 7766, 1023, 1109, 279, 15899, 11, 902, 690, 387, 6646, 15899, 422, 54691, 4936, 23783, 1202, 2671, 33549, 311, 11360, 220, 22, 1618, 1073, 382, 14711, 14924, 25, 578, 2457, 315, 279, 5226, 271, 14711, 16533, 25, 128000, 12, 220, 22, 339, 1938, 315, 6250, 11, 220, 2550, 24, 13, 128001]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 128000, 12, 220, 22, 339, 1938, 315, 6250, 11, 220, 2550, 24, 13, 128001]\n",
      "<|begin_of_text|>You are a helpful assistant. Review the contract clauses and answer questions. Output the mentioned clauses if exist; otherwise output \"No\".\n",
      "\n",
      "###Clauses:\n",
      "EXHIBIT 10.6\n",
      " DISTRIBUTOR AGREEMENT\n",
      " THIS DISTRIBUTOR AGREEMENT (the \"Agreement\") is made by and between Electric City Corp., a Delaware corporation (\"Company\") and Electric City of Illinois LLC (\"Distributor\") this 7th day of September, 1999.\n",
      " RECITALS\n",
      " A. The Company's Business. The Company is presently engaged in the business of selling an energy efficiency device, which is referred to as an \"Energy Saver\" which may be improved or otherwise changed from its present composition (the \"Products\"). The Company may engage in the business of selling other products or other devices other than the Products, which will be considered Products if Distributor exercises its options pursuant to Section 7 hereof.\n",
      "\n",
      "###Question: The date of the contract\n",
      "\n",
      "###Answer:<|begin_of_text|>- 7th day of September, 1999.<|end_of_text|>\n",
      "<|begin_of_text|>- 7th day of September, 1999.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "print(sample.keys())\n",
    "print(sample['input_ids'])\n",
    "print(sample['labels'])\n",
    "print(tokenizer.decode(sample['input_ids']))\n",
    "print(tokenizer.decode([k for k in sample['labels'] if k != -100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', '-', 'Ġ', '7', 'th', 'Ġday', 'Ġof', 'ĠSeptember', ',', 'Ġ', '199', '9', '.', '<|end_of_text|>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([k for k in sample['labels'] if k != -100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from cache: data/ood_split/seed42_tr29/llama3/pmt_01/cache/cached_test_data_id.jsonl_Meta-Llama-3-8B_v1.0.pkl\n"
     ]
    }
   ],
   "source": [
    "generator = SimpleGenerator(tokenizer, is_encoder_decoder=is_seq2seq)\n",
    "\n",
    "test_args = Namespace(\n",
    "    data_path = f'data/ood_split/seed42_tr29/{tk_name}/pmt_01/test_data_id.jsonl',\n",
    "    max_length = 1000,\n",
    "    debug = False,\n",
    ")\n",
    "test_ds = load_test_dataset(test_args, tokenizer, is_seq2seq, part = 'sampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_batch(data):\n",
    "    return {\n",
    "        'input_ids': torch.tensor(data['input_ids']).unsqueeze(0).cuda(),\n",
    "        'attention_mask': torch.tensor(data['attention_mask']).unsqueeze(0).cuda(),\n",
    "        'labels': torch.tensor(data['labels']).unsqueeze(0).cuda(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>You are a helpful assistant. Review the contract clauses and answer questions. Output the mentioned clauses if exist; otherwise output \"No\".\\n\\n###Clauses:\\n3\\nSource: LOHA CO. LTD., F-1, 12/9/2019\\n12.2 (1) Invoice in 3 originals indicating contract number and L/C number. (2) Final acceptance certificate signed by the Buyer and the Seller. 13. SHIPMENT: CIP The seller shall contract on usual terms at his own expenses for the carriage of the goods to the agreed point at the named place of destination and bear all risks and expenses until the goods have been delivered to the port of destination. The Sellers shall ship the goods within the shipment time from the port of shipment to the port of destination. Transshipment is allowed. Partial Shipment is allowed. In case the goods are to be dispatched by parcel post/sea-freight, the Sellers shall, 3 days before the time of delivery, inform the Buyers by cable/letter of the estimated date of delivery, Contract No., commodity, invoiced value, etc. The sellers shall, immediately after dispatch of the goods, advise the Buyers by cable/letter of the Contract No., commodity, invoiced value and date of dispatch for the Buyers. 14. SHIPPING ADVICE: The seller shall within 72 hours after the shipment of the goods, advise the shipping department of buyer by fax or E-mail of Contract No., goods name, quantity, value, number of packages, gross weight, measurements and the estimated arrival time of the goods at the destination. 15. GUARANTEE OF QUALITY: The Sellers guarantee that the commodity hereof is complies in all respects with the quality and specification stipulated in this Contract. 16. CLAIMS: Within 7 days after the arrival of the goods at destination, should the quality, specification, or quantity be found not in conformity with the stipulations of the Contract except those claims for which the insurance company or the owners of the vessel are liable, the Buyers, on the strength of the Inspection Certificate issued by the China Commodity Inspection Bureau, have the right to claim for replacement with new goods, or for compensation, and all the expenses (such as inspection charges, freight for returning the goods and for sending the replacement, insurance premium, storage and loading and unloading charges etc.) shall be borne by the Sellers. The Certificate so issued shall be accepted as the base of a claim. The Sellers, in accordance with the Buyers\\' claim, shall be responsible for the immediate elimination of the defect(s), complete or partial replacement of the commodity or shall devaluate the commodity according\\n\\n###Question: The date of the contract\\n\\n###Answer:'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test_ds[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = generator(model, to_batch(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_tokens': [tensor([128000,   2822, 128001], device='cuda:0')]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>No<|end_of_text|>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(r['pred_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6665, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model(**to_batch(sample)).loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_idx = sample['labels'].index(128000)\n",
    "new_tgt = r['pred_tokens'][0].tolist()\n",
    "new_ids = sample['input_ids'][:tgt_idx] + new_tgt\n",
    "new_mask = [1] * len(new_ids)\n",
    "new_labels = [-100] * tgt_idx + new_tgt\n",
    "neg_sample = {'input_ids': new_ids, 'attention_mask': new_mask, 'labels': new_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0191, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.decode(neg_sample['input_ids']))\n",
    "with torch.no_grad():\n",
    "    print(model(**to_batch(neg_sample)).loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7 hereof.\n",
      "\n",
      "###Question: The date of the contract\n",
      "\n",
      "###Answer:<|begin_of_text|>- \n"
     ]
    }
   ],
   "source": [
    "# Force model to predict a span.\n",
    "tgt_len = len([k for k in sample['labels'] if k != -100])\n",
    "new_len = len(sample['input_ids']) - tgt_len + 3 # 3 is the first three token\n",
    "pos_sample = {k:v[:new_len] for k,v in sample.items()}\n",
    "print(tokenizer.decode(pos_sample['input_ids'][-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pos = generator(model, to_batch(pos_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7th day of September, 1999<|end_of_text|>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(r_pos['pred_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_head = [12800, 12]\n",
    "neg_head = [12800, 2822]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target_head(sample, head):\n",
    "    \"\"\"Append target head to inputs\"\"\"\n",
    "    ori_ids = sample['input_ids']\n",
    "    ipt_len = len([k for k in sample['labels'] if k == -100])\n",
    "    new_ids = ori_ids[:ipt_len] + list(head)\n",
    "    new_mask = [1] * len(new_ids)\n",
    "    new_labels = [-100] * ipt_len + list(head)\n",
    "    return {'input_ids': new_ids, 'attention_mask': new_mask, 'labels': new_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pos_batch = to_batch(add_target_head(sample, [12800]))\n",
    "    pos_out = model(**pos_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_of_text|> 128001: 0.2812570631504059\n",
      "<|begin_of_text|> 128000: 0.13285644352436066\n",
      "Ġon 389: 0.10346870124340057\n",
      "Cla 65217: 0.07111292332410812\n",
      "Ġ 220: 0.06275693327188492\n",
      "No 2822: 0.045913953334093094\n",
      "Ġclauses 50198: 0.03155616670846939\n",
      "ĠNo 2360: 0.011608866043388844\n",
      "Ġthe 279: 0.010244788601994514\n",
      "Ġfor 369: 0.005837304051965475\n"
     ]
    }
   ],
   "source": [
    "probs = torch.softmax(pos_out.logits[0,-1], dim = 0)\n",
    "top = torch.argsort(probs, descending = True)\n",
    "for i in range(10):\n",
    "    token = top[i].item()\n",
    "    print(f'{tokenizer.convert_ids_to_tokens(token)} {token}: {probs[token]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage_fast/rhshui/lib/anaconda3/envs/llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/storage_fast/rhshui/lib/anaconda3/envs/llm/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   2675,    527,    264,  11190,  18328,     13,  10506,    279,\n",
       "           5226,  50198,    323,   4320,   4860,     13,   9442,    279,   9932,\n",
       "          50198,    422,   3073,     26,   6062,   2612,    330,   2822,  11690,\n",
       "          14711,  65217,   4881,    512,   3337,     39,   3336,    964,    220,\n",
       "            605,     13,     21,    198,  98941,    878,  76590,  16837,    198,\n",
       "          10245,  98941,    878,  76590,  16837,    320,   1820,    330,   9219,\n",
       "          17589,    909,    374,   1903,    555,    323,   1990,  21246,   4409,\n",
       "          22621,   2637,    264,  40838,  27767,   3573,  14831,    909,    323,\n",
       "          21246,   4409,    315,  19174,  15620,   3573,     35,  79488,    909,\n",
       "            420,    220,     22,    339,   1938,    315,   6250,     11,    220,\n",
       "           2550,     24,    627,  75236,  34288,     50,    198,    362,     13,\n",
       "            578,   8351,    596,   8184,     13,    578,   8351,    374,  50801,\n",
       "          17045,    304,    279,   2626,    315,  11486,    459,   4907,  15374,\n",
       "           3756,     11,    902,    374,  14183,    311,    439,    459,    330,\n",
       "          33775,    328,   7403,      1,    902,   1253,    387,  13241,    477,\n",
       "           6062,   5614,    505,   1202,   3118,  18528,    320,   1820,    330,\n",
       "          18219,   1865,    578,   8351,   1253,  16988,    304,    279,   2626,\n",
       "            315,  11486,   1023,   3956,    477,   1023,   7766,   1023,   1109,\n",
       "            279,  15899,     11,    902,    690,    387,   6646,  15899,    422,\n",
       "          54691,   4936,  23783,   1202,   2671,  33549,    311,  11360,    220,\n",
       "             22,   1618,   1073,    382,  14711,  14924,     25,    578,   2457,\n",
       "            315,    279,   5226,    271,  14711,  16533,     25, 128000,   2822,\n",
       "         128001]], device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipt = add_target_head(sample, [])\n",
    "model.generate(torch.tensor(ipt['input_ids']).unsqueeze(0).cuda(), do_sample = False, eos_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġpursuant', 'Ġto', 'ĠSection', 'Ġ', '7', 'Ġhere', 'of', '.ĊĊ', '###', 'Question', ':', 'ĠThe', 'Ġdate', 'Ġof', 'Ġthe', 'Ġcontract', 'ĊĊ', '###', 'Answer', ':']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(ipt['input_ids'][-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
