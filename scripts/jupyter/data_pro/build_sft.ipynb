{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "os.environ['HF_HUB_CACHE'] = '/next_share/hf_cache/hub/'\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "import importlib\n",
    "import numpy as np\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "import context\n",
    "os.chdir(context.proj_dir)\n",
    "\n",
    "import cont_gen\n",
    "import cont_gen.data_process.ood.build_src_tgt\n",
    "import cont_gen.data_process.ood.build_sft_meta_data\n",
    "importlib.reload(cont_gen.data_process.ood.build_src_tgt)\n",
    "importlib.reload(cont_gen.data_process.ood.build_sft_meta_data)\n",
    "from cont_gen.data_process.ood.build_src_tgt import process, SFT_Builder, SFT_Builder_YesNo, SFT_Builder_YesNo_Natural\n",
    "from cont_gen.data_process.ood.build_sft_meta_data import CUAD_Basic, MetaSFT_Train_Builder, MetaSFT_Test_Builder\n",
    "from cont_gen.data_loader.cuad_sft import CUAD_SFT_Cached\n",
    "from cont_gen.utils import load_jsonl, save_jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- Get train and test label sets.\n",
    "  - Files: `train_labels.csv`, `test_labels.csv`\n",
    "    - Keys: `['clause_id', 'clause_type']`\n",
    "- [Link](#build-meta-data) For each tokenizer's data, build train and test meta data. (`data/ood_split/{split_name}/{tokenizer_name}`)\n",
    "  - Input para data: `data/cuad_clean/merge_split/paras_{tokenizer_name}_512.jsonl`\n",
    "  - Files: `train_meta.csv`, `test_meta_ood.csv`, `test_meta_id.csv`\n",
    "    - Keys: `['title', 'para_idx', 'q_id', 'answers', 'type']`\n",
    "- [Link](#build-sft-data) For each tokenizer and prompt method, build source and target data and save to `data/ood_split/{split_name}/{tokenizer_name}/{prompt_name}`\n",
    "  - Files: `train_data.jsonl`, `test_data_ood.jsonl`, `test_data_id.jsonl`\n",
    "    - Keys: `['title', 'para_idx', 'q_id', 'source', 'target']`\n",
    "- [Link](#chat-data) Data for Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_meta(train_para_data, train_labels, output_dir, neg_clause_ratio=1.0, num_neg_quest = 1):\n",
    "    \"\"\"Build and save train meta data.\"\"\"\n",
    "    all_df = MetaSFT_Train_Builder.build_pos_neg_samples(\n",
    "        train_para_data,\n",
    "        train_labels,\n",
    "        neg_clause_ratio=neg_clause_ratio,\n",
    "        num_neg_quest=num_neg_quest)\n",
    "\n",
    "    Path(output_dir).mkdir(parents = True, exist_ok=True)\n",
    "    all_df.to_csv(Path(output_dir) / 'train_meta.csv', index = False)\n",
    "\n",
    "    return all_df\n",
    "\n",
    "def build_test_meta(test_para_data, test_labels, train_labels, output_dir, neg_ratio = 0.1):\n",
    "    \"\"\"\n",
    "    Build and save test meta data.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents = True, exist_ok=True)\n",
    "    # OOD test\n",
    "    test_df = MetaSFT_Test_Builder.build_test_and_small(test_para_data, test_labels, neg_ratio = neg_ratio)\n",
    "    \n",
    "    test_df.to_csv(Path(output_dir) / 'test_meta_ood.csv', index = False)\n",
    "\n",
    "    # ID test\n",
    "    test_id_df = MetaSFT_Test_Builder.build_test_and_small(test_para_data, train_labels, neg_ratio = neg_ratio)\n",
    "    \n",
    "    test_id_df.to_csv(Path(output_dir) / 'test_meta_id.csv', index = False)\n",
    "\n",
    "    return test_df, test_id_df\n",
    "\n",
    "def process_meta_tokenizer(tkn_name, split_dirs, proj_dir = './'):\n",
    "    \"\"\"\n",
    "    Build meta data for one tokenizer under multiple splits\n",
    "    \"\"\"\n",
    "    proj_dir = Path(proj_dir)\n",
    "    cuad_basic = CUAD_Basic(\n",
    "        proj_dir / 'data/clause/all_info.csv',\n",
    "        proj_dir / f'data/cuad_clean/merge_split/paras_{tkn_name}_512.jsonl',\n",
    "        proj_dir / 'data/cuad_split/ori_train_titles.json',\n",
    "        proj_dir / 'data/cuad_split/ori_test_titles.json',\n",
    "    )\n",
    "    for split_dir in split_dirs:\n",
    "        split_dir = Path(split_dir)\n",
    "        train_labels = pd.read_csv(split_dir / 'train_labels.csv')['clause_id'].to_list()\n",
    "        test_labels = pd.read_csv(split_dir / 'test_labels.csv')['clause_id'].to_list()\n",
    "        build_train_meta(cuad_basic.train_para_data, train_labels, split_dir / tkn_name)\n",
    "        build_test_meta(cuad_basic.test_para_data, test_labels, train_labels, split_dir / tkn_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('data/ood_split/seed42_tr29'), PosixPath('data/ood_split/seed128_tr29'), PosixPath('data/ood_split/seed89_tr29')]\n"
     ]
    }
   ],
   "source": [
    "print(list(Path('data/ood_split/').glob('*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle tokenizer data: flan-t5\n",
      "Handle tokenizer data: llama2\n",
      "Handle tokenizer data: llama3\n",
      "Handle tokenizer data: mistral\n",
      "Handle tokenizer data: phi1\n",
      "Handle tokenizer data: phi2\n"
     ]
    }
   ],
   "source": [
    "tkn_names = ['flan-t5', 'llama2', 'llama3', 'mistral', 'phi2']\n",
    "\n",
    "split_names = ['seed42_tr29', 'seed128_tr29', 'seed89_tr29']\n",
    "\n",
    "for tkn_name in tkn_names:\n",
    "    print(f'Handle tokenizer data: {tkn_name}')\n",
    "    process_meta_tokenizer(tkn_name, [f'data/ood_split/{k}' for k in split_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'para_idx', 'q_id', 'answers', 'type'], dtype='object')\n",
      "Index(['title', 'para_idx', 'q_id', 'answers', 'type'], dtype='object')\n",
      "Train: 15692, Test OOD: 67188, Test ID: 162371\n",
      "type\n",
      "0    5734\n",
      "1    5734\n",
      "2    4224\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "0    55380\n",
      "3     6132\n",
      "2     5093\n",
      "1      583\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get some statistics\n",
    "tkn_name = tkn_names[2]\n",
    "split = split_names[0]\n",
    "train_meta = pd.read_csv(f'data/ood_split/{split}/{tkn_name}/train_meta.csv')\n",
    "test_meta_ood = pd.read_csv(f'data/ood_split/{split}/{tkn_name}/test_meta_ood.csv')\n",
    "test_meta_id = pd.read_csv(f'data/ood_split/{split}/{tkn_name}/test_meta_id.csv')\n",
    "print(train_meta.columns)\n",
    "print(test_meta_ood.columns)\n",
    "print(f'Train: {len(train_meta)}, Test OOD: {len(test_meta_ood)}, Test ID: {len(test_meta_id)}')\n",
    "print(train_meta['type'].value_counts())\n",
    "print(test_meta_ood['type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build SFT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def process_sft_tokenizer(tkn_name, split_dirs, builder: SFT_Builder, pmt_name):\n",
    "    \"\"\"Build meta data for one tokenizer under multiple splits\"\"\"\n",
    "    all_para_data = load_jsonl(f'data/cuad_clean/merge_split/paras_{tkn_name}_512.jsonl')\n",
    "    builder.set_para_data(all_para_data)\n",
    "\n",
    "    for split in split_dirs:\n",
    "        print(f'Process {split}')\n",
    "        meta_dir = Path(split) / tkn_name\n",
    "        save_dir = meta_dir / pmt_name\n",
    "        train_meta = pd.read_csv(meta_dir / 'train_meta.csv', converters={'answers': literal_eval})\n",
    "        train_data  = process(builder, train_meta)\n",
    "        save_jsonl(train_data, save_dir / 'train_data.jsonl')\n",
    "\n",
    "        test_meta_id = pd.read_csv(meta_dir / 'test_meta_id.csv', converters={'answers': literal_eval})\n",
    "        test_data_id  = process(builder, test_meta_id)\n",
    "        save_jsonl(test_data_id, save_dir / 'test_data_id.jsonl')\n",
    "\n",
    "        test_meta_ood = pd.read_csv(meta_dir / 'test_meta_ood.csv', converters={'answers': literal_eval})\n",
    "        test_data_ood  = process(builder, test_meta_ood)\n",
    "        save_jsonl(test_data_ood, save_dir / 'test_data_ood.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle tokenizer data: flan-t5\n",
      "Process data/ood_split/seed42_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15760/15760 [00:00<00:00, 69887.99it/s]\n",
      "100%|██████████| 164749/164749 [00:02<00:00, 72025.91it/s]\n",
      "100%|██████████| 68172/68172 [00:00<00:00, 73251.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed128_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14427/14427 [00:00<00:00, 33567.59it/s]\n",
      "100%|██████████| 164749/164749 [00:02<00:00, 71222.98it/s]\n",
      "100%|██████████| 68172/68172 [00:00<00:00, 73235.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed89_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16548/16548 [00:00<00:00, 68615.79it/s]\n",
      "100%|██████████| 164749/164749 [00:03<00:00, 45737.71it/s]\n",
      "100%|██████████| 68172/68172 [00:01<00:00, 41914.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle tokenizer data: llama2\n",
      "Process data/ood_split/seed42_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15951/15951 [00:00<00:00, 68605.58it/s]\n",
      "100%|██████████| 167591/167591 [00:02<00:00, 71996.07it/s]\n",
      "100%|██████████| 69348/69348 [00:00<00:00, 72200.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed128_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14598/14598 [00:00<00:00, 69199.55it/s]\n",
      "100%|██████████| 167591/167591 [00:02<00:00, 71684.12it/s]\n",
      "100%|██████████| 69348/69348 [00:01<00:00, 42295.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed89_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16719/16719 [00:00<00:00, 67865.40it/s]\n",
      "100%|██████████| 167591/167591 [00:03<00:00, 46311.66it/s]\n",
      "100%|██████████| 69348/69348 [00:01<00:00, 43955.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle tokenizer data: llama3\n",
      "Process data/ood_split/seed42_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15692/15692 [00:00<00:00, 69742.33it/s]\n",
      "100%|██████████| 162371/162371 [00:02<00:00, 71902.33it/s]\n",
      "100%|██████████| 67188/67188 [00:00<00:00, 73169.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed128_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14364/14364 [00:00<00:00, 32788.94it/s]\n",
      "100%|██████████| 162371/162371 [00:02<00:00, 71719.96it/s]\n",
      "100%|██████████| 67188/67188 [00:01<00:00, 42879.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed89_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16484/16484 [00:00<00:00, 68036.44it/s]\n",
      "100%|██████████| 162371/162371 [00:02<00:00, 71710.40it/s]\n",
      "100%|██████████| 67188/67188 [00:01<00:00, 41461.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle tokenizer data: mistral\n",
      "Process data/ood_split/seed42_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15812/15812 [00:00<00:00, 69851.10it/s]\n",
      "100%|██████████| 166083/166083 [00:02<00:00, 71738.88it/s]\n",
      "100%|██████████| 68724/68724 [00:00<00:00, 72884.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed128_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14495/14495 [00:00<00:00, 68420.07it/s]\n",
      "100%|██████████| 166083/166083 [00:02<00:00, 72069.25it/s]\n",
      "100%|██████████| 68724/68724 [00:00<00:00, 72882.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed89_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16599/16599 [00:00<00:00, 67757.91it/s]\n",
      "100%|██████████| 166083/166083 [00:02<00:00, 70876.73it/s]\n",
      "100%|██████████| 68724/68724 [00:01<00:00, 42491.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle tokenizer data: phi1\n",
      "Process data/ood_split/seed42_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15719/15719 [00:00<00:00, 69933.91it/s]\n",
      "100%|██████████| 162342/162342 [00:02<00:00, 72209.52it/s]\n",
      "100%|██████████| 67176/67176 [00:01<00:00, 45154.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed128_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14376/14376 [00:00<00:00, 69214.18it/s]\n",
      "100%|██████████| 162342/162342 [00:02<00:00, 71913.78it/s]\n",
      "100%|██████████| 67176/67176 [00:00<00:00, 72131.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed89_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16497/16497 [00:00<00:00, 67033.68it/s]\n",
      "100%|██████████| 162342/162342 [00:03<00:00, 45891.35it/s]\n",
      "100%|██████████| 67176/67176 [00:00<00:00, 72228.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle tokenizer data: phi2\n",
      "Process data/ood_split/seed42_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15719/15719 [00:00<00:00, 37682.40it/s]\n",
      "100%|██████████| 162342/162342 [00:02<00:00, 72154.71it/s]\n",
      "100%|██████████| 67176/67176 [00:00<00:00, 73326.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed128_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14376/14376 [00:00<00:00, 69160.51it/s]\n",
      "100%|██████████| 162342/162342 [00:03<00:00, 46881.36it/s]\n",
      "100%|██████████| 67176/67176 [00:00<00:00, 71887.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data/ood_split/seed89_tr29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16497/16497 [00:00<00:00, 68046.58it/s]\n",
      "100%|██████████| 162342/162342 [00:02<00:00, 71585.25it/s]\n",
      "100%|██████████| 67176/67176 [00:01<00:00, 42980.93it/s]\n"
     ]
    }
   ],
   "source": [
    "tkn_names = ['flan-t5', 'llama2', 'llama3', 'mistral', 'phi2']\n",
    "\n",
    "split_names = ['seed42_tr29', 'seed128_tr29', 'seed89_tr29']\n",
    "\n",
    "clause_info = pd.read_csv('./data/clause/all_info.csv')\n",
    "\n",
    "prompt_01 = open('config/prompts/pmt_01.txt', 'r').read()\n",
    "\n",
    "BUILDER_MAP = {\n",
    "    'pmt_01': SFT_Builder(prompt_01, clause_info, None, lambda k: k),\n",
    "    'pmt_01_yes_no': SFT_Builder_YesNo(prompt_01, clause_info, None, lambda k: k)\n",
    "}\n",
    "\n",
    "bd_name = 'pmt_01_yes_no' # customize\n",
    "\n",
    "for tkn_name in tkn_names:\n",
    "    print(f'Handle tokenizer data: {tkn_name}')\n",
    "    process_sft_tokenizer(\n",
    "        tkn_name, [f'data/ood_split/{k}' for k in split_names], \n",
    "        BUILDER_MAP[bd_name], bd_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'para_idx', 'q_id', 'source', 'target', 'type'])\n",
      "dict_keys(['title', 'para_idx', 'q_id', 'source', 'target', 'type'])\n",
      "{'title': 'LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGREEMENT', 'para_idx': 0, 'q_id': 2, 'source': 'You are a helpful assistant. Review the contract clauses and answer questions. Output the mentioned clauses if exist; otherwise output \"No\".\\n\\n###Clauses:\\nEXHIBIT 10.6\\n DISTRIBUTOR AGREEMENT\\n THIS DISTRIBUTOR AGREEMENT (the \"Agreement\") is made by and between Electric City Corp., a Delaware corporation (\"Company\") and Electric City of Illinois LLC (\"Distributor\") this 7th day of September, 1999.\\n RECITALS\\n A. The Company\\'s Business. The Company is presently engaged in the business of selling an energy efficiency device, which is referred to as an \"Energy Saver\" which may be improved or otherwise changed from its present composition (the \"Products\"). The Company may engage in the business of selling other products or other devices other than the Products, which will be considered Products if Distributor exercises its options pursuant to Section 7 hereof.\\n\\n###Question: The date of the contract\\n\\n###Answer:', 'target': 'Yes. - 7th day of September, 1999.', 'type': 0}\n"
     ]
    }
   ],
   "source": [
    "# Show SFT Data\n",
    "tkn_name = tkn_names[2]\n",
    "split = split_names[0]\n",
    "bd_name = 'pmt_01_yes_no'\n",
    "train_data = load_jsonl(f'data/ood_split/{split}/{tkn_name}/{bd_name}/train_data.jsonl')\n",
    "test_data_ood = load_jsonl(f'data/ood_split/{split}/{tkn_name}/{bd_name}/test_data_ood.jsonl')\n",
    "test_data_id = load_jsonl(f'data/ood_split/{split}/{tkn_name}/{bd_name}/test_data_id.jsonl')\n",
    "print(train_data[0].keys())\n",
    "print(test_data_ood[0].keys())\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "def build_tkn(path):\n",
    "    return AutoTokenizer.from_pretrained(path, trust_remote_code = True)\n",
    "\n",
    "TKN_MAP = {'flan-t5': build_tkn('google/flan-t5-large'),\n",
    "    'llama2': build_tkn('meta-llama/Llama-2-7b-hf'),\n",
    "    'llama3': build_tkn('meta-llama/Meta-Llama-3-8B'),\n",
    "    'mistral': build_tkn('mistralai/Mistral-7B-v0.1'),\n",
    "    # 'phi1': build_tkn('microsoft/phi-1_5'),\n",
    "    'phi2': build_tkn('microsoft/phi-2')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "tk = TKN_MAP['llama3']\n",
    "print(tk.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "# path = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "msg = [\n",
    "    #  {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "r = tokenizer.apply_chat_template(msg, tokenize=False, \n",
    "        add_generation_prompt=True)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 200/200 [00:00<00:00, 1123.16it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/ood_split/seed42_tr29/llama3/pmt_01/train_data.jsonl'\n",
    "train_ds = CUAD_SFT_Cached(\n",
    "    data_path, tokenizer, is_seq2seq=False, is_chat=True, small = True\n",
    "    # cache_dir = Path(data_path).parent / 'cache',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant. Review the contract clauses and answer questions. Output the mentioned clauses if exist; otherwise output \"No\".\n",
      "\n",
      "###Clauses:\n",
      "EXHIBIT 10.6\n",
      " DISTRIBUTOR AGREEMENT\n",
      " THIS DISTRIBUTOR AGREEMENT (the \"Agreement\") is made by and between Electric City Corp., a Delaware corporation (\"Company\") and Electric City of Illinois LLC (\"Distributor\") this 7th day of September, 1999.\n",
      " RECITALS\n",
      " A. The Company's Business. The Company is presently engaged in the business of selling an energy efficiency device, which is referred to as an \"Energy Saver\" which may be improved or otherwise changed from its present composition (the \"Products\"). The Company may engage in the business of selling other products or other devices other than the Products, which will be considered Products if Distributor exercises its options pursuant to Section 7 hereof.\n",
      "\n",
      "###Question: The date of the contract\n",
      "\n",
      "###Answer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "- 7th day of September, 1999.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_ds[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁The', '▁date', '▁of', '▁the', '▁contract', '<0x0A>', '<0x0A>', '###', 'An', 'swer', ':', '▁[', '/', 'INST', ']', '▁-', '▁', '7', 'th', '▁day', '▁of', '▁September', ',', '▁', '1', '9', '9', '9', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(train_ds[0]['input_ids'][-30:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1127"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(k['input_ids']) for k in train_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 733, 16289, 28793, 6526, 460, 368, 28804, 733, 28748, 16289, 28793]\n"
     ]
    }
   ],
   "source": [
    "r2 = tokenizer.apply_chat_template(msg, tokenize=True, \n",
    "        add_generation_prompt=True)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁[', 'INST', ']', '▁Who', '▁are', '▁you', '?', '▁[', '/', 'INST', ']']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
