{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import importlib\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "\n",
    "import context\n",
    "os.chdir(context.proj_dir)\n",
    "\n",
    "import cont_gen\n",
    "import cont_gen.data_process.pre_process.split_long_para\n",
    "importlib.reload(cont_gen.data_process.pre_process.split_long_para)\n",
    "from cont_gen.data_process.pre_process.split_long_para import process_doc_split_paras, split_to_chunk_span\n",
    "from cont_gen.utils import load_jsonl, save_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "def build_tkn(path):\n",
    "    return AutoTokenizer.from_pretrained(\n",
    "        path, cache_dir = '/next_share/hf_cache/hub', trust_remote_code = True\n",
    "    )\n",
    "\n",
    "# Load data\n",
    "all_docs = load_jsonl('./data/cuad_clean/CUADv1_paras_merge_new.jsonl')\n",
    "\n",
    "# tokenizer name to path\n",
    "TKN_MAP = {\n",
    "    'flan-t5': build_tkn('google/flan-t5-large'),\n",
    "    'llama2': build_tkn('meta-llama/Llama-2-7b-hf'),\n",
    "    'llama3': build_tkn('meta-llama/Meta-Llama-3-8B'),\n",
    "    'mistral': build_tkn('mistralai/Mistral-7B-v0.1'),\n",
    "    # 'phi1': build_tkn('microsoft/phi-1_5'),\n",
    "    'phi2': build_tkn('microsoft/phi-2')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:09<00:00, 53.04it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# save_data = [process_doc_split_paras(d, TKN_MAP['llama3'], 512) for d in tqdm(all_docs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process flan-t5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/510 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 510/510 [00:09<00:00, 52.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:11<00:00, 44.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process llama3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:09<00:00, 51.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process mistral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:11<00:00, 44.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process phi1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:09<00:00, 54.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process phi2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:09<00:00, 56.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process with all tokenizers\n",
    "tk_names = TKN_MAP.keys()\n",
    "save_path_tpl = 'data/cuad_clean/merge_split/paras_{tk_name}_512.jsonl'\n",
    "for tk_name in tk_names:\n",
    "    print(f'Process {tk_name}')\n",
    "    save_data = [process_doc_split_paras(d, TKN_MAP[tk_name], 512) for d in tqdm(all_docs)]\n",
    "    save_jsonl(save_data, save_path_tpl.format(tk_name = tk_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_one_answer(context, answer):\n",
    "    \"\"\"make sure the the position of one answer is correct\"\"\"\n",
    "    a_text = answer['text']\n",
    "    pos_span = context[answer['start_pos']: answer['end_pos'] + 1]\n",
    "    return a_text == pos_span\n",
    "\n",
    "def check_answers_of_para(para_data):\n",
    "    context = para_data['text']\n",
    "    answers = [a for k in para_data['qas'] for a in k['answers']]\n",
    "    res = [check_one_answer(context ,a) for a in answers]\n",
    "    return all(res)\n",
    "\n",
    "def sim_score(a, b):\n",
    "    r = difflib.SequenceMatcher(a = a, b = b)\n",
    "    return r.ratio()\n",
    "\n",
    "def check_all_answer(pro_data):\n",
    "    results = []\n",
    "    for p_data in pro_data:\n",
    "        doc_r = [check_answers_of_para(k) for k in p_data['paras']]\n",
    "        results.append(all(doc_r))\n",
    "    print('Error(anser match span): ', len([k for k in results if not k]))\n",
    "    return results\n",
    "\n",
    "def check_all_doc(all_docs, pro_data):\n",
    "    \"\"\"To check the joint of split paragraphs are equal to original paragraph\"\"\"\n",
    "    results = []\n",
    "    for ori_doc, p_doc in zip(all_docs, pro_data):\n",
    "        ori_paras = [k['text'] for k in ori_doc['paras']]\n",
    "        split_paras = [[] for _ in ori_paras]\n",
    "        for para in p_doc['paras']:\n",
    "            split_paras[para['old_para_idx']].append(para['text'])\n",
    "        split_paras = [''.join(k) for k in split_paras]\n",
    "\n",
    "        para_r = [a == b for a,b in zip(ori_paras, split_paras)]\n",
    "        \n",
    "        results.append(para_r)\n",
    "    print('Doc match < 1.0: ', len([k for k in results if not all(k)]))\n",
    "    return results\n",
    "\n",
    "def check_doc_clause(ori_doc, pro_data):\n",
    "    def extract_answers(doc_data):\n",
    "        q2anws = defaultdict(list)\n",
    "        for para in doc_data['paras']:\n",
    "            for qa in para['qas']:\n",
    "                anws = [k['text'] for k in qa['answers']]\n",
    "                q2anws[qa['q_id']].extend(anws)\n",
    "        return q2anws\n",
    "    \n",
    "    q2as_ori = extract_answers(ori_doc)\n",
    "    q2as_pro = extract_answers(pro_data)\n",
    "    assert len(q2as_ori) == len(q2as_pro)\n",
    "\n",
    "    q2sim = {}\n",
    "    for q_id in q2as_ori:\n",
    "        q2sim[q_id] = sim_score(\n",
    "            ''.join(q2as_ori[q_id]),\n",
    "            ''.join(q2as_pro[q_id]),\n",
    "        )\n",
    "    return q2sim\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check flan-t5\n",
      "Error(anser match span):  0\n",
      "Doc match < 1.0:  0\n",
      "Error (clause):  0\n",
      "Check llama2\n",
      "Error(anser match span):  0\n",
      "Doc match < 1.0:  0\n",
      "Error (clause):  0\n",
      "Check llama3\n",
      "Error(anser match span):  0\n",
      "Doc match < 1.0:  0\n",
      "Error (clause):  0\n",
      "Check mistral\n",
      "Error(anser match span):  0\n",
      "Doc match < 1.0:  1\n",
      "Error (clause):  1\n",
      "Check phi1\n",
      "Error(anser match span):  0\n",
      "Doc match < 1.0:  0\n",
      "Error (clause):  0\n",
      "Check phi2\n",
      "Error(anser match span):  0\n",
      "Doc match < 1.0:  0\n",
      "Error (clause):  0\n"
     ]
    }
   ],
   "source": [
    "for tk_name in TKN_MAP.keys():\n",
    "    print(f'Check {tk_name}')\n",
    "    save_data = load_jsonl(save_path_tpl.format(tk_name  = tk_name))\n",
    "\n",
    "    r = check_all_answer(save_data)\n",
    "    doc_match_r = check_all_doc(all_docs, save_data)\n",
    "    all_q2sim = [check_doc_clause(ori_d, new_d) for ori_d, new_d in zip(all_docs, save_data)]\n",
    "    clause_r = [all([v == 1.0 for v in k.values()]) for k in all_q2sim]\n",
    "    print('Error (clause): ', len([k for k in clause_r if not k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = load_jsonl(save_path_tpl.format(tk_name  = 'mistral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc match < 1.0:  1\n"
     ]
    }
   ],
   "source": [
    "doc_match_r = check_all_doc(all_docs, save_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "[0]\n",
      "[2138, 2018, 1428]\n",
      "[True, True, True]\n"
     ]
    }
   ],
   "source": [
    "error_idx = [i for i,k in enumerate(doc_match_r) if not all(k)]\n",
    "\n",
    "ci = error_idx[0]\n",
    "print(ci)\n",
    "\n",
    "pi = [i for i,k in enumerate(doc_match_r[ci]) if not k]\n",
    "print(pi)\n",
    "\n",
    "ori_ctx = all_docs[ci]['paras'][pi[0]]['text']\n",
    "sp_ctxs = [p['text'] for p in save_data[ci]['paras'] if p['old_para_idx'] == pi[0]]\n",
    "\n",
    "print([len(k) for k in sp_ctxs])\n",
    "print([k in ori_ctx for k in sp_ctxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Demand (\"          \"). The Auctio\n",
      "Target Demand (\"    \n",
      "       \"). The Aucti\n"
     ]
    }
   ],
   "source": [
    "print(ori_ctx[2138 - 20: 2138 + 20])\n",
    "print(sp_ctxs[0][-20:])\n",
    "print(sp_ctxs[1][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Target', '▁Dem', 'and', '▁(\"', '<0xE2>', '<0x80>', '<0xAF>', '<0xE2>', '<0x80>', '<0xAF>', '<0xE2>', '<0x80>', '<0xAF>', '<0xE2>', '<0x80>', '<0xAF>', '<0xE2>', '<0x80>', '<0xAF>', '<0xE2>', '<0x80>', '<0xAF>', '<0xE2>', '<0x80>', '<0xAF>', '<0xE2>', '<0x80>', '<0xAF>', '<0xE2>', '<0x80>', '<0xAF>', '<0xE2>', '<0x80>', '<0xAF>', '\").', '▁The', '▁A', 'uct', 'io']\n"
     ]
    }
   ],
   "source": [
    "print(TKN_MAP['mistral'].tokenize(ori_ctx[2138 - 20: 2138 + 20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Tokenizer\n",
    "\n",
    "To check whether phi1 and phi2's tokenizers are same.\n",
    "\n",
    "Conclusion: they are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_vocab(vocab1, vocab2):\n",
    "    inv_1 = {v:k for k,v in vocab1.items()}\n",
    "    inv_2 = {v:k for k,v in vocab2.items()}\n",
    "    tot = max(len(inv_1), len(inv_2))\n",
    "    match_r = [inv_1.get(i, None) == inv_2.get(i, None) for i in range(tot)]\n",
    "    return [i for i, m in enumerate(match_r) if not m]\n",
    "\n",
    "def compare_tokenizer(tk1, tk2):\n",
    "    print(tk1.vocab_size)\n",
    "    print(tk2.vocab_size)\n",
    "    not_match = compare_vocab(tk1.vocab, tk2.vocab)\n",
    "    print(len(not_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "128000\n",
      "128250\n"
     ]
    }
   ],
   "source": [
    "compare_tokenizer(TKN_MAP['llama2'], TKN_MAP['llama3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
